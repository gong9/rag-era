"""
LightRAG ÊúçÂä° - FastAPI ÂÖ•Âè£
Êèê‰æõÁü•ËØÜÂõæË∞±Â¢ûÂº∫ÁöÑ RAG Ê£ÄÁ¥¢ËÉΩÂäõ
"""

import os
import asyncio
import logging
from typing import Optional, List
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx

from config import (
    OPENAI_API_KEY,
    OPENAI_API_BASE,
    OPENAI_MODEL,
    LIGHTRAG_STORAGE_DIR,
    SERVICE_HOST,
    SERVICE_PORT,
    LOG_LEVEL,
    INDEX_DELAY_SECONDS,
    LLM_CONCURRENCY,
)

# Ê≥®ÊÑèÔºö‰∏çË¶ÅÂú® uvicorn ÁéØÂ¢É‰∏ã‰ΩøÁî® nest_asyncio.apply()
# ÂÆÉ‰∏é uvloop ‰∏çÂÖºÂÆπ

# ÈÖçÁΩÆÊó•Âøó
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# LightRAG ÂÆû‰æãÁºìÂ≠òÔºàÊåâÁü•ËØÜÂ∫ì IDÔºâ
rag_instances: dict = {}

# Á¥¢Âºï‰ªªÂä°Áä∂ÊÄÅ
indexing_tasks: dict = {}

# LLM ËØ∑Ê±ÇÂπ∂ÂèëÈôêÂà∂Ôºà‰ø°Âè∑ÈáèÔºâÔºå0 Ë°®Á§∫‰∏çÈôêÂà∂
llm_semaphore = asyncio.Semaphore(LLM_CONCURRENCY) if LLM_CONCURRENCY > 0 else None


# ========== Ëá™ÂÆö‰πâ LLM ÂáΩÊï∞Ôºà‰ΩøÁî®ÂçÉÈóÆ APIÔºâ==========


async def qwen_complete(
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List[dict] = [],
    **kwargs,
) -> str:
    """
    Ë∞ÉÁî®ÂçÉÈóÆ API ÂÆåÊàêÊñáÊú¨ÁîüÊàê
    """
    messages = []

    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    for msg in history_messages:
        messages.append(msg)

    messages.append({"role": "user", "content": prompt})

    async def do_request():
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OPENAI_API_BASE}/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": OPENAI_MODEL,
                    "messages": messages,
                    "temperature": 0.7,
                    "max_tokens": 4096,
                },
            )

            if response.status_code != 200:
                logger.error(f"LLM API error: {response.status_code} - {response.text}")
                raise HTTPException(
                    status_code=500, detail=f"LLM API error: {response.text}"
                )

            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Â¶ÇÊûúËÆæÁΩÆ‰∫ÜÂπ∂ÂèëÈôêÂà∂Ôºå‰ΩøÁî®‰ø°Âè∑Èáè
    if llm_semaphore:
        async with llm_semaphore:
            return await do_request()
    else:
        return await do_request()


async def qwen_embedding(texts: List[str]) -> List[List[float]]:
    """
    Ë∞ÉÁî®ÂçÉÈóÆ Embedding API
    """

    async def do_request():
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OPENAI_API_BASE}/embeddings",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": "text-embedding-v3",
                    "input": texts,
                },
            )

            if response.status_code != 200:
                logger.error(
                    f"Embedding API error: {response.status_code} - {response.text}"
                )
                raise HTTPException(
                    status_code=500, detail=f"Embedding API error: {response.text}"
                )

            data = response.json()
            return [item["embedding"] for item in data["data"]]

    # Â¶ÇÊûúËÆæÁΩÆ‰∫ÜÂπ∂ÂèëÈôêÂà∂Ôºå‰ΩøÁî®‰ø°Âè∑Èáè
    if llm_semaphore:
        async with llm_semaphore:
            return await do_request()
    else:
        return await do_request()


# ========== Pydantic Ê®°Âûã ==========


class IndexRequest(BaseModel):
    kb_id: str
    documents: List[dict]  # [{"id": "xxx", "name": "xxx", "content": "xxx"}]


class QueryRequest(BaseModel):
    kb_id: str
    question: str
    mode: str = "hybrid"  # local, global, hybrid, naive


class IndexStatus(BaseModel):
    kb_id: str
    status: str  # pending, indexing, completed, failed
    progress: float = 0.0
    message: str = ""


# ========== LightRAG ÂÆû‰æãÁÆ°ÁêÜ ==========


def get_storage_path(kb_id: str) -> str:
    """Ëé∑ÂèñÁü•ËØÜÂ∫ìÁöÑÂ≠òÂÇ®Ë∑ØÂæÑ"""
    return os.path.join(LIGHTRAG_STORAGE_DIR, f"kb_{kb_id}")


async def get_or_create_rag(kb_id: str):
    """Ëé∑ÂèñÊàñÂàõÂª∫ LightRAG ÂÆû‰æã"""
    if kb_id in rag_instances:
        return rag_instances[kb_id]

    try:
        from lightrag import LightRAG, QueryParam

        storage_path = get_storage_path(kb_id)
        os.makedirs(storage_path, exist_ok=True)

        # ÂàõÂª∫ LightRAG ÂÆû‰æã
        # Ê≥®ÊÑèÔºöLightRAG ÈúÄË¶ÅËá™ÂÆö‰πâ LLM ÂáΩÊï∞
        rag = LightRAG(
            working_dir=storage_path,
            llm_model_func=qwen_complete,
            embedding_func=EmbeddingFunc(
                embedding_dim=1024,  # text-embedding-v3 ÁöÑÁª¥Â∫¶
                max_token_size=8192,
                func=qwen_embedding,
            ),
        )

        # ÂàùÂßãÂåñÂ≠òÂÇ®ÔºàLightRAG 1.4+ ÂøÖÈúÄÔºâ
        await rag.initialize_storages()

        # ÂàùÂßãÂåñ pipeline Áä∂ÊÄÅ
        from lightrag.kg.shared_storage import initialize_pipeline_status

        await initialize_pipeline_status()

        rag_instances[kb_id] = rag
        logger.info(f"Created LightRAG instance for kb: {kb_id}")
        return rag
    except ImportError as e:
        logger.error(f"Failed to import LightRAG: {e}")
        raise HTTPException(status_code=500, detail="LightRAG not installed")
    except Exception as e:
        logger.error(f"Failed to create LightRAG instance: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========== FastAPI Â∫îÁî® ==========


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Â∫îÁî®ÁîüÂëΩÂë®ÊúüÁÆ°ÁêÜ"""
    logger.info("LightRAG service starting...")
    os.makedirs(LIGHTRAG_STORAGE_DIR, exist_ok=True)
    yield
    logger.info("LightRAG service shutting down...")


app = FastAPI(
    title="LightRAG Service",
    description="Áü•ËØÜÂõæË∞±Â¢ûÂº∫ÁöÑ RAG Ê£ÄÁ¥¢ÊúçÂä°",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS ÈÖçÁΩÆ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ========== API Ë∑ØÁî± ==========


@app.get("/health")
async def health_check():
    """ÂÅ•Â∫∑Ê£ÄÊü•"""
    return {
        "status": "healthy",
        "service": "lightrag",
        "storage_dir": LIGHTRAG_STORAGE_DIR,
        "instances": len(rag_instances),
    }


@app.post("/index")
async def index_documents(request: IndexRequest, background_tasks: BackgroundTasks):
    """
    Á¥¢ÂºïÊñáÊ°£ÔºàÊûÑÂª∫Áü•ËØÜÂõæË∞±Ôºâ
    ËøôÊòØ‰∏Ä‰∏™ÂºÇÊ≠•Êìç‰ΩúÔºåËøîÂõû‰ªªÂä° ID ÂêéÂú®ÂêéÂè∞ÊâßË°å
    """
    kb_id = request.kb_id
    documents = request.documents

    if not documents:
        raise HTTPException(status_code=400, detail="No documents provided")

    # Ê£ÄÊü•ÊòØÂê¶Â∑≤Âú®Á¥¢Âºï‰∏≠
    if kb_id in indexing_tasks and indexing_tasks[kb_id]["status"] == "indexing":
        return {
            "status": "already_indexing",
            "kb_id": kb_id,
            "message": "Indexing already in progress",
        }

    # ÂàùÂßãÂåñ‰ªªÂä°Áä∂ÊÄÅ
    indexing_tasks[kb_id] = {
        "status": "pending",
        "progress": 0.0,
        "message": "Starting indexing...",
        "total": len(documents),
        "completed": 0,
    }

    # Âú®ÂêéÂè∞ÊâßË°åÁ¥¢Âºï
    background_tasks.add_task(index_documents_task, kb_id, documents)

    return {
        "status": "accepted",
        "kb_id": kb_id,
        "message": f"Indexing {len(documents)} documents in background",
    }


async def index_documents_task(kb_id: str, documents: List[dict]):
    """
    ÂêéÂè∞Á¥¢Âºï‰ªªÂä°
    ‰ΩøÁî®Âª∂ËøüÂíåÈôêÊµÅÈÅøÂÖç CPU ËøáËΩΩÔºå‰øùËØÅÊúçÂä°Âô®ÂèØÁî®ÊÄß
    """
    try:
        indexing_tasks[kb_id]["status"] = "indexing"
        indexing_tasks[kb_id]["message"] = "Creating knowledge graph..."

        rag = await get_or_create_rag(kb_id)
        total = len(documents)

        logger.info(
            f"[{kb_id}] Starting indexing with delay={INDEX_DELAY_SECONDS}s, concurrency={LLM_CONCURRENCY}"
        )

        for i, doc in enumerate(documents):
            content = doc.get("content", "")
            name = doc.get("name", f"doc_{i}")

            if not content:
                continue

            # Ê∑ªÂä†ÊñáÊ°£Ê†áËØÜ
            text_with_meta = f"„ÄêÊñáÊ°£: {name}„Äë\n\n{content}"

            # ÊèíÂÖ•Âà∞ LightRAGÔºàÊûÑÂª∫Áü•ËØÜÂõæË∞±Ôºâ
            await rag.ainsert(text_with_meta)

            # Êõ¥Êñ∞ËøõÂ∫¶
            indexing_tasks[kb_id]["completed"] = i + 1
            indexing_tasks[kb_id]["progress"] = (i + 1) / total
            indexing_tasks[kb_id]["message"] = f"Indexed {i + 1}/{total}: {name}"
            logger.info(f"[{kb_id}] Indexed {i + 1}/{total}: {name}")

            # üî• ÊØè‰∏™ÊñáÊ°£Â§ÑÁêÜÂêéÂª∂ËøüÔºåÈÅøÂÖç CPU ÊåÅÁª≠Êª°ËΩΩ
            # ËøôÊ†∑ÂÖ∂‰ªñÊúçÂä°ÔºàÂ¶Ç Next.jsÔºâÂèØ‰ª•Ëé∑Âæó CPU Êó∂Èó¥
            if INDEX_DELAY_SECONDS > 0 and i < total - 1:
                logger.debug(
                    f"[{kb_id}] Throttling: sleeping {INDEX_DELAY_SECONDS}s..."
                )
                await asyncio.sleep(INDEX_DELAY_SECONDS)

        indexing_tasks[kb_id]["status"] = "completed"
        indexing_tasks[kb_id]["progress"] = 1.0
        indexing_tasks[kb_id]["message"] = f"Successfully indexed {total} documents"
        logger.info(f"[{kb_id}] Indexing completed: {total} documents")

    except Exception as e:
        logger.error(f"[{kb_id}] Indexing failed: {e}")
        indexing_tasks[kb_id]["status"] = "failed"
        indexing_tasks[kb_id]["message"] = str(e)


@app.get("/index/{kb_id}/status")
async def get_index_status(kb_id: str):
    """Ëé∑ÂèñÁ¥¢ÂºïÁä∂ÊÄÅ"""
    if kb_id not in indexing_tasks:
        # Ê£ÄÊü•ÊòØÂê¶Â∑≤ÊúâÂ≠òÂÇ®
        storage_path = get_storage_path(kb_id)
        if os.path.exists(storage_path):
            return {
                "kb_id": kb_id,
                "status": "completed",
                "progress": 1.0,
                "message": "Index exists",
            }
        return {
            "kb_id": kb_id,
            "status": "not_found",
            "progress": 0.0,
            "message": "No indexing task found",
        }

    return {
        "kb_id": kb_id,
        **indexing_tasks[kb_id],
    }


@app.post("/query")
async def query(request: QueryRequest):
    """
    Êü•ËØ¢Áü•ËØÜÂ∫ìÔºàÂõæË∞±Ê£ÄÁ¥¢Ôºâ

    mode ÂèÇÊï∞:
    - local: Âü∫‰∫éÂÆû‰ΩìÁöÑÂ±ÄÈÉ®Ê£ÄÁ¥¢ÔºàÈÄÇÂêàÂÖ∑‰ΩìÈóÆÈ¢òÔºâ
    - global: Âü∫‰∫é‰∏ªÈ¢òÁöÑÂÖ®Â±ÄÊ£ÄÁ¥¢ÔºàÈÄÇÂêàÊÄªÁªìÊÄßÈóÆÈ¢òÔºâ
    - hybrid: Ê∑∑ÂêàÊ®°ÂºèÔºàÊé®ËçêÔºâ
    - naive: ÁÆÄÂçïÂêëÈáèÊ£ÄÁ¥¢ÔºàÂØπÁÖßÁªÑÔºâ
    """
    kb_id = request.kb_id
    question = request.question
    mode = request.mode

    if not question:
        raise HTTPException(status_code=400, detail="Question is required")

    # Ê£ÄÊü•Áü•ËØÜÂ∫ìÊòØÂê¶Â≠òÂú®
    storage_path = get_storage_path(kb_id)
    if not os.path.exists(storage_path):
        raise HTTPException(status_code=404, detail=f"Knowledge base {kb_id} not found")

    try:
        from lightrag import QueryParam

        rag = await get_or_create_rag(kb_id)

        # ÊâßË°åÊü•ËØ¢
        logger.info(f"[{kb_id}] Query: '{question}' (mode: {mode})")

        result = await rag.aquery(question, param=QueryParam(mode=mode))

        logger.info(f"[{kb_id}] Query result length: {len(result)} chars")

        return {
            "kb_id": kb_id,
            "question": question,
            "mode": mode,
            "answer": result,
        }

    except Exception as e:
        logger.error(f"[{kb_id}] Query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/index/{kb_id}")
async def delete_index(kb_id: str):
    """Âà†Èô§Áü•ËØÜÂ∫ìÁ¥¢Âºï"""
    import shutil

    storage_path = get_storage_path(kb_id)

    # ‰ªéÁºìÂ≠òÁßªÈô§
    if kb_id in rag_instances:
        del rag_instances[kb_id]

    if kb_id in indexing_tasks:
        del indexing_tasks[kb_id]

    # Âà†Èô§Â≠òÂÇ®ÁõÆÂΩï
    if os.path.exists(storage_path):
        shutil.rmtree(storage_path)
        logger.info(f"Deleted index for kb: {kb_id}")
        return {"status": "deleted", "kb_id": kb_id}

    return {"status": "not_found", "kb_id": kb_id}


@app.get("/indexes")
async def list_indexes():
    """ÂàóÂá∫ÊâÄÊúâÁü•ËØÜÂ∫ìÁ¥¢Âºï"""
    indexes = []

    if os.path.exists(LIGHTRAG_STORAGE_DIR):
        for name in os.listdir(LIGHTRAG_STORAGE_DIR):
            if name.startswith("kb_"):
                kb_id = name[3:]  # ÂéªÊéâ "kb_" ÂâçÁºÄ
                storage_path = os.path.join(LIGHTRAG_STORAGE_DIR, name)
                indexes.append(
                    {
                        "kb_id": kb_id,
                        "path": storage_path,
                        "cached": kb_id in rag_instances,
                    }
                )

    return {"indexes": indexes, "total": len(indexes)}


@app.get("/graph/{kb_id}")
async def get_graph(kb_id: str, limit: int = 100):
    """
    Ëé∑ÂèñÁü•ËØÜÂõæË∞±Êï∞ÊçÆÔºàÂÆû‰ΩìÂíåÂÖ≥Á≥ªÔºâ
    Áî®‰∫éÂâçÁ´ØÂèØËßÜÂåñÂ±ïÁ§∫
    """
    import xml.etree.ElementTree as ET

    storage_path = get_storage_path(kb_id)

    # Â¶ÇÊûúÁü•ËØÜÂ∫ìÁõÆÂΩï‰∏çÂ≠òÂú®ÔºåËøîÂõûÁ©∫Êï∞ÊçÆÔºàËÄå‰∏çÊòØ 404Ôºâ
    if not os.path.exists(storage_path):
        return {
            "kb_id": kb_id,
            "entities": [],
            "relations": [],
            "message": "Áü•ËØÜÂõæË∞±Â∞öÊú™ÊûÑÂª∫ÔºåËØ∑ÂÖàÁÇπÂáª„ÄåÊûÑÂª∫Áü•ËØÜÂõæË∞±„ÄçÊåâÈíÆ",
            "stats": {"entity_count": 0, "relation_count": 0},
        }

    entities = []
    relations = []
    entity_map = {}

    try:
        # ËØªÂèñ GraphML Êñá‰ª∂ÔºàLightRAG ÁöÑÂÆûÈôÖÂ≠òÂÇ®Ê†ºÂºèÔºâ
        graphml_file = os.path.join(storage_path, "graph_chunk_entity_relation.graphml")

        if os.path.exists(graphml_file):
            logger.info(f"[{kb_id}] Reading GraphML file: {graphml_file}")
            tree = ET.parse(graphml_file)
            root = tree.getroot()

            # GraphML ÂëΩÂêçÁ©∫Èó¥
            ns = {"graphml": "http://graphml.graphdrawing.org/xmlns"}

            # ÊâæÂà∞ graph ÂÖÉÁ¥†ÔºàÂ∞ùËØïÂ∏¶ÂëΩÂêçÁ©∫Èó¥Âíå‰∏çÂ∏¶ÂëΩÂêçÁ©∫Èó¥Ôºâ
            graph = root.find(".//graphml:graph", ns)
            if graph is None:
                graph = root.find(".//{http://graphml.graphdrawing.org/xmlns}graph")
            if graph is None:
                graph = root.find(".//graph")

            if graph is not None:
                # Ëß£ÊûêËäÇÁÇπÔºàÂÆû‰ΩìÔºâ
                nodes = graph.findall(".//{http://graphml.graphdrawing.org/xmlns}node")
                if not nodes:
                    nodes = graph.findall(".//node")

                for node in nodes:
                    node_id = node.get("id", "")
                    if not node_id:
                        continue

                    # Ëé∑ÂèñËäÇÁÇπÂ±ûÊÄß
                    entity_type = "ENTITY"
                    description = ""

                    for data in node.findall(
                        ".//{http://graphml.graphdrawing.org/xmlns}data"
                    ) or node.findall(".//data"):
                        key = data.get("key", "")
                        text = (data.text or "").strip()
                        if key == "entity_type" or key == "d0":
                            entity_type = text.upper() if text else "ENTITY"
                        elif key == "description" or key == "d1":
                            description = text

                    entity_map[node_id] = {
                        "id": node_id,
                        "name": node_id,
                        "type": entity_type,
                        "description": description,
                    }

                # Ëß£ÊûêËæπÔºàÂÖ≥Á≥ªÔºâ
                edges = graph.findall(".//{http://graphml.graphdrawing.org/xmlns}edge")
                if not edges:
                    edges = graph.findall(".//edge")

                for edge in edges:
                    source = edge.get("source", "")
                    target = edge.get("target", "")

                    if not source or not target:
                        continue

                    # Ëé∑ÂèñÂÖ≥Á≥ªÁ±ªÂûã
                    rel_type = "RELATED"
                    description = ""

                    for data in edge.findall(
                        ".//{http://graphml.graphdrawing.org/xmlns}data"
                    ) or edge.findall(".//data"):
                        key = data.get("key", "")
                        text = (data.text or "").strip()
                        if key in ("relation_type", "d2", "d4") and text:
                            rel_type = text
                        elif key in ("description", "d3", "d5"):
                            description = text

                    relations.append(
                        {
                            "source": source,
                            "target": target,
                            "type": rel_type,
                            "description": description,
                        }
                    )

                entities = list(entity_map.values())
                logger.info(
                    f"[{kb_id}] Loaded {len(entities)} entities, {len(relations)} relations from GraphML"
                )

        # Â¶ÇÊûúÊ≤°ÊúâÊï∞ÊçÆÔºåËøîÂõûÊèêÁ§∫
        if not entities and not relations:
            files = os.listdir(storage_path) if os.path.exists(storage_path) else []
            logger.warning(f"[{kb_id}] No graph data. Storage files: {files}")
            return {
                "kb_id": kb_id,
                "entities": [],
                "relations": [],
                "message": "Áü•ËØÜÂõæË∞±Ê≠£Âú®ÊûÑÂª∫‰∏≠ÊàñÊûÑÂª∫Â§±Ë¥•„ÄÇËØ∑Á®çÂêéÂà∑Êñ∞ÈáçËØïÔºåÊàñÈáçÊñ∞ÁÇπÂáª„ÄåÊûÑÂª∫Áü•ËØÜÂõæË∞±„ÄçÊåâÈíÆ„ÄÇ",
                "stats": {"entity_count": 0, "relation_count": 0},
            }

        # ÈôêÂà∂ËøîÂõûÊï∞Èáè
        entities = entities[:limit]
        relations = relations[:limit]

        return {
            "kb_id": kb_id,
            "entities": entities,
            "relations": relations,
            "stats": {
                "entity_count": len(entities),
                "relation_count": len(relations),
            },
        }

    except Exception as e:
        logger.error(f"[{kb_id}] Failed to read graph: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========== Embedding ÂáΩÊï∞ÂåÖË£ÖÂô® ==========


class EmbeddingFunc:
    """Embedding ÂáΩÊï∞ÂåÖË£ÖÂô®ÔºåÁ¨¶Âêà LightRAG Ë¶ÅÊ±ÇÁöÑÊé•Âè£"""

    def __init__(self, embedding_dim: int, max_token_size: int, func):
        self.embedding_dim = embedding_dim
        self.max_token_size = max_token_size
        self.func = func  # LightRAG ÊúüÊúõ .func Â±ûÊÄß

    async def __call__(self, texts: List[str]) -> List[List[float]]:
        return await self.func(texts)


# ========== ÂêØÂä®ÂÖ•Âè£ ==========

if __name__ == "__main__":
    import uvicorn

    logger.info(f"Starting LightRAG service on {SERVICE_HOST}:{SERVICE_PORT}")
    logger.info(f"Storage directory: {LIGHTRAG_STORAGE_DIR}")
    logger.info(f"LLM Model: {OPENAI_MODEL}")

    uvicorn.run(
        "main:app",
        host=SERVICE_HOST,
        port=SERVICE_PORT,
        reload=False,
        log_level=LOG_LEVEL.lower(),
    )
